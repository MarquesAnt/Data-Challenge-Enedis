{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13741897,"sourceType":"datasetVersion","datasetId":8743715}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm\nfrom pathlib import Path","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:12:04.474753Z","iopub.execute_input":"2025-11-15T15:12:04.475110Z","iopub.status.idle":"2025-11-15T15:12:04.480190Z","shell.execute_reply.started":"2025-11-15T15:12:04.475088Z","shell.execute_reply":"2025-11-15T15:12:04.479108Z"}},"outputs":[],"execution_count":181},{"cell_type":"markdown","source":"## Importation des données d'entraînement et de test","metadata":{}},{"cell_type":"code","source":"DATA_PATH = Path(\"../input/preprocessed\")\nX_tr = pd.read_csv(DATA_PATH / \"X_train.csv\", index_col=0,parse_dates=True)\nX_test = pd.read_csv(DATA_PATH / \"X_test.csv\", index_col=0,parse_dates=True)\nY_tr = pd.read_csv(DATA_PATH / \"y_train.csv\", index_col=0,parse_dates=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:12:04.481805Z","iopub.execute_input":"2025-11-15T15:12:04.482069Z","iopub.status.idle":"2025-11-15T15:12:39.849380Z","shell.execute_reply.started":"2025-11-15T15:12:04.482047Z","shell.execute_reply":"2025-11-15T15:12:39.848734Z"}},"outputs":[],"execution_count":182},{"cell_type":"code","source":"X_tr = np.expm1(X_tr)\nX_test = np.expm1(X_test)\nY_tr = np.expm1(Y_tr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:12:39.850165Z","iopub.execute_input":"2025-11-15T15:12:39.850379Z","iopub.status.idle":"2025-11-15T15:12:39.968816Z","shell.execute_reply.started":"2025-11-15T15:12:39.850362Z","shell.execute_reply":"2025-11-15T15:12:39.967909Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: invalid value encountered in expm1\n  result = func(self.values, **kwargs)\n","output_type":"stream"}],"execution_count":183},{"cell_type":"code","source":"holed_cols = [c for c in X_tr.columns if c.startswith(\"holed\")]\nclean_cols = [c for c in X_tr.columns if c not in holed_cols]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:12:39.969898Z","iopub.execute_input":"2025-11-15T15:12:39.970159Z","iopub.status.idle":"2025-11-15T15:12:40.206918Z","shell.execute_reply.started":"2025-11-15T15:12:39.970141Z","shell.execute_reply":"2025-11-15T15:12:40.206245Z"}},"outputs":[],"execution_count":184},{"cell_type":"code","source":"assert list(holed_cols) == list(Y_tr.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:12:40.208660Z","iopub.execute_input":"2025-11-15T15:12:40.208963Z","iopub.status.idle":"2025-11-15T15:12:40.212829Z","shell.execute_reply.started":"2025-11-15T15:12:40.208944Z","shell.execute_reply":"2025-11-15T15:12:40.212234Z"}},"outputs":[],"execution_count":185},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"class Config:\n    hidden_size = 256\n    num_layers = 3\n    dropout = 0.2\n    batch_size = 64\n    learning_rate = 0.001\n    num_epochs = 50\n    patience = 10\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nconfig = Config()\nprint(f\"Using device: {config.device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:12:40.213810Z","iopub.execute_input":"2025-11-15T15:12:40.214608Z","iopub.status.idle":"2025-11-15T15:12:40.230452Z","shell.execute_reply.started":"2025-11-15T15:12:40.214584Z","shell.execute_reply":"2025-11-15T15:12:40.229631Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":186},{"cell_type":"markdown","source":"## Classes","metadata":{}},{"cell_type":"markdown","source":"## Fonctions d'entraînement","metadata":{}},{"cell_type":"code","source":"# ============= Dataset =============\n\nclass TimeSeriesDataset(Dataset):\n    \"\"\"Dataset pour les séries temporelles avec valeurs manquantes\"\"\"\n    \n    def __init__(self, X, y=None, scaler=None, fit_scaler=False):\n        \"\"\"\n        X: DataFrame (timestamps × courbes)\n        y: DataFrame avec les vraies valeurs (même format que X)\n        \"\"\"\n        self.X = X.values.T  # Shape: (n_series, n_timesteps)\n        self.y = y.values.T if y is not None else None\n        \n        # Normalisation\n        if fit_scaler:\n            self.scaler = StandardScaler()\n            # Fit sur les valeurs non-NaN uniquement\n            valid_values = self.X[~np.isnan(self.X)].reshape(-1, 1)\n            self.scaler.fit(valid_values)\n        else:\n            self.scaler = scaler\n            \n        # Normaliser X\n        self.X_scaled = self.X.copy()\n        mask = ~np.isnan(self.X)\n        self.X_scaled[mask] = self.scaler.transform(self.X[mask].reshape(-1, 1)).flatten()\n        \n        # Remplacer les NaN par 0 pour le modèle (on utilisera le mask)\n        self.X_scaled = np.nan_to_num(self.X_scaled, nan=0.0)\n        \n        # Normaliser Y aussi !\n        if self.y is not None:\n            self.y_scaled = self.scaler.transform(self.y.reshape(-1, 1)).reshape(self.y.shape)\n        else:\n            self.y_scaled = None\n        \n        # Créer le masque (1 = valeur observée, 0 = manquante)\n        self.mask = (~np.isnan(self.X)).astype(np.float32)\n\n    def __len__(self):\n        return self.X.shape[0]  # Nombre de séries\n    \n    def __getitem__(self, idx):\n        x = torch.FloatTensor(self.X_scaled[idx]).unsqueeze(-1)  # (timesteps, 1)\n        mask = torch.FloatTensor(self.mask[idx]).unsqueeze(-1)   # (timesteps, 1)\n        \n        if self.y_scaled is not None:\n            y = torch.FloatTensor(self.y_scaled[idx]).unsqueeze(-1)\n            return x, mask, y\n        return x, mask\n\n# ============= Modèle BiLSTM =============\n\nclass BiLSTMImputer(nn.Module):\n    \"\"\"Modèle BiLSTM bidirectionnel pour imputation\"\"\"\n    \n    def __init__(self, input_size=1, hidden_size=128, num_layers=2, dropout=0.2):\n        super().__init__()\n        \n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # BiLSTM\n        self.bilstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            dropout=dropout if num_layers > 1 else 0,\n            bidirectional=True,\n            batch_first=True\n        )\n        \n        # Couche de sortie\n        self.fc = nn.Linear(hidden_size * 2, 1)  # *2 car bidirectionnel\n        \n    def forward(self, x, mask):\n        \"\"\"\n        x: (batch, timesteps, 1)\n        mask: (batch, timesteps, 1)\n        \"\"\"\n        # Concaténer x et mask comme input\n        x_masked = torch.cat([x, mask], dim=-1)  # (batch, timesteps, 2)\n        \n        # Passer par le BiLSTM\n        lstm_out, _ = self.bilstm(x_masked)  # (batch, timesteps, hidden*2)\n        \n        # Prédiction\n        output = self.fc(lstm_out)  # (batch, timesteps, 1)\n        \n        return output\n\n\n# ============= Loss personnalisée =============\n\nclass MaskedMSELoss(nn.Module):\n    \"\"\"MSE loss qui ne calcule l'erreur que sur les valeurs manquantes\"\"\"\n    \n    def forward(self, pred, target, mask):\n        \"\"\"\n        pred: prédictions (batch, timesteps, 1)\n        target: vraies valeurs (batch, timesteps, 1)\n        mask: masque (batch, timesteps, 1) - 1=observé, 0=manquant\n        \"\"\"\n        # On veut prédire uniquement les valeurs manquantes\n        missing_mask = (1 - mask)  # Inverser le masque\n        \n        # Calculer l'erreur uniquement sur les valeurs manquantes\n        error = (pred - target) ** 2\n        masked_error = error * missing_mask\n        \n        # Moyenne sur les valeurs manquantes uniquement\n        n_missing = missing_mask.sum()\n        if n_missing > 0:\n            loss = masked_error.sum() / n_missing\n        else:\n            loss = torch.tensor(0.0, device=pred.device)\n        \n        return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:12:40.231291Z","iopub.execute_input":"2025-11-15T15:12:40.231518Z","iopub.status.idle":"2025-11-15T15:12:40.246653Z","shell.execute_reply.started":"2025-11-15T15:12:40.231496Z","shell.execute_reply":"2025-11-15T15:12:40.245960Z"}},"outputs":[],"execution_count":187},{"cell_type":"code","source":"def train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    \n    for x, mask, y in tqdm(loader, desc=\"Training\", leave=False):\n        x, mask, y = x.to(device), mask.to(device), y.to(device)\n        \n        optimizer.zero_grad()\n        \n        # Forward\n        pred = model(x, mask)\n        loss = criterion(pred, y, mask)\n        \n        # Backward\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(loader)\n\ndef validate(model, loader, criterion, device, scaler):\n    model.eval()\n    total_loss = 0\n    total_mae = 0\n    \n    with torch.no_grad():\n        for x, mask, y in loader:\n            x, mask, y = x.to(device), mask.to(device), y.to(device)\n            \n            pred = model(x, mask)\n            loss = criterion(pred, y, mask)\n            \n            # Calculer MAE sur les vraies valeurs (dénormalisées)\n            pred_denorm = scaler.inverse_transform(pred.cpu().numpy().reshape(-1, 1))\n            y_denorm = scaler.inverse_transform(y.cpu().numpy().reshape(-1, 1))\n            mask_cpu = mask.cpu().numpy().reshape(-1, 1)\n            \n            # MAE seulement sur les valeurs manquantes\n            missing_mask = (1 - mask_cpu).astype(bool).flatten()\n            if missing_mask.sum() > 0:\n                mae = np.abs(pred_denorm.flatten()[missing_mask] - \n                           y_denorm.flatten()[missing_mask]).mean()\n                total_mae += mae\n            \n            total_loss += loss.item()\n    \n    return total_loss / len(loader), total_mae / len(loader)\n\ndef train_model(model, train_loader, val_loader, config, scaler):\n    \"\"\"Entraînement avec early stopping\"\"\"\n    \n    criterion = MaskedMSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n    )\n    \n    best_val_loss = float('inf')\n    patience_counter = 0\n    \n    for epoch in range(config.num_epochs):\n        # Train\n        train_loss = train_epoch(model, train_loader, optimizer, criterion, config.device)\n        \n        # Validate\n        val_loss, val_mae = validate(model, val_loader, criterion, config.device, scaler)\n        \n        # Scheduler\n        scheduler.step(val_loss)\n        \n        print(f\"Epoch {epoch+1}/{config.num_epochs}\")\n        print(f\"  Train Loss: {train_loss:.4f}\")\n        print(f\"  Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.2f}\")\n        \n        # Early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            # Sauvegarder le meilleur modèle\n            torch.save(model.state_dict(), 'best_bilstm_model.pt')\n            print(\"  → New best model saved!\")\n        else:\n            patience_counter += 1\n            \n        if patience_counter >= config.patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n    \n    # Charger le meilleur modèle\n    model.load_state_dict(torch.load('best_bilstm_model.pt'))\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:12:40.247393Z","iopub.execute_input":"2025-11-15T15:12:40.247573Z","iopub.status.idle":"2025-11-15T15:12:40.269943Z","shell.execute_reply.started":"2025-11-15T15:12:40.247558Z","shell.execute_reply":"2025-11-15T15:12:40.269250Z"}},"outputs":[],"execution_count":188},{"cell_type":"markdown","source":"## Fonctions de prédictions","metadata":{}},{"cell_type":"code","source":"def predict(model, X_df, scaler, config, batch_size=64):\n    \"\"\"Prédire les valeurs manquantes\"\"\"\n    \n    # Créer dataset\n    dataset = TimeSeriesDataset(X_df, scaler=scaler, fit_scaler=False)\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    \n    model.eval()\n    all_predictions = []\n    \n    with torch.no_grad():\n        for x, mask in tqdm(loader, desc=\"Predicting\"):\n            x, mask = x.to(config.device), mask.to(config.device)\n            \n            pred = model(x, mask)\n            all_predictions.append(pred.cpu().numpy())\n    \n    # Concatener et reshape\n    predictions = np.concatenate(all_predictions, axis=0)  # (n_series, timesteps, 1)\n    predictions = predictions.squeeze(-1).T  # (timesteps, n_series)\n    \n    # Dénormaliser\n    predictions_denorm = scaler.inverse_transform(predictions.reshape(-1, 1))\n    predictions_denorm = predictions_denorm.reshape(predictions.shape)\n    \n    # Créer DataFrame\n    pred_df = pd.DataFrame(\n        predictions_denorm,\n        index=X_df.index,\n        columns=X_df.columns\n    )\n    \n    # Remplir uniquement les valeurs manquantes\n    result = X_df.copy()\n    mask = X_df.isna()\n    result[mask] = pred_df[mask]\n    \n    return result\n\n# ============= Fonction principale =============\ndef run_bilstm_imputation(X_tr, Y_tr, holed_cols, clean_cols, config):\n    \"\"\"\n    Pipeline complet d'entraînement et prédiction\n    \n    Returns:\n        model: modèle entraîné\n        scaler: scaler utilisé\n        train_loader: pour analyse\n        val_loader: pour analyse\n    \"\"\"\n    \n    # Séparer train/val (80/20 sur les courbes holed)\n    n_holed = len(holed_cols)\n    n_train = int(0.8 * n_holed)\n    \n    train_cols = holed_cols[:n_train]\n    val_cols = holed_cols[n_train:]\n    \n    print(f\"Training on {len(train_cols)} series, validating on {len(val_cols)} series\")\n    \n    # Créer datasets\n    train_dataset = TimeSeriesDataset(\n        X_tr[train_cols], \n        Y_tr[train_cols], \n        fit_scaler=True\n    )\n    \n    val_dataset = TimeSeriesDataset(\n        X_tr[val_cols], \n        Y_tr[val_cols], \n        scaler=train_dataset.scaler\n    )\n    \n    # DataLoaders\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=config.batch_size, \n        shuffle=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=config.batch_size, \n        shuffle=False\n    )\n    \n    # Créer modèle\n    model = BiLSTMImputer(\n        input_size=2,  # valeur + mask\n        hidden_size=config.hidden_size,\n        num_layers=config.num_layers,\n        dropout=config.dropout\n    ).to(config.device)\n    \n    print(f\"\\nModel architecture:\")\n    print(model)\n    print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    \n    # Entraîner\n    print(\"\\nStarting training...\")\n    model = train_model(model, train_loader, val_loader, config, train_dataset.scaler)\n    \n    return model, train_dataset.scaler, train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:12:40.270639Z","iopub.execute_input":"2025-11-15T15:12:40.270879Z","iopub.status.idle":"2025-11-15T15:12:40.286718Z","shell.execute_reply.started":"2025-11-15T15:12:40.270863Z","shell.execute_reply":"2025-11-15T15:12:40.286145Z"}},"outputs":[],"execution_count":189},{"cell_type":"markdown","source":"## Pré-entraînement","metadata":{}},{"cell_type":"code","source":"# ============= PRÉ-ENTRAÎNEMENT =============\n\ndef create_masked_data(X_clean, mask_prob=0.15, seed=42):\n    \"\"\"\n    Créer des données synthétiques avec masques aléatoires\n    pour le pré-entraînement\n    \n    Args:\n        X_clean: DataFrame avec courbes complètes\n        mask_prob: probabilité de masquer une valeur (15% par défaut)\n        seed: graine aléatoire\n    \n    Returns:\n        X_masked: DataFrame avec valeurs masquées (NaN)\n        Y_true: DataFrame avec vraies valeurs (pour la supervision)\n    \"\"\"\n    np.random.seed(seed)\n    \n    X_masked = X_clean.copy()\n    Y_true = X_clean.copy()\n    \n    print(f\"Création de données masquées (mask_prob={mask_prob})...\")\n    \n    # Pour chaque courbe, masquer aléatoirement des timesteps\n    for col in tqdm(X_masked.columns, desc=\"Masking\"):\n        # Nombre de valeurs à masquer\n        n_values = len(X_masked)\n        n_to_mask = int(n_values * mask_prob)\n        \n        # Sélectionner aléatoirement les indices à masquer\n        mask_indices = np.random.choice(n_values, size=n_to_mask, replace=False)\n        \n        # Masquer\n        X_masked.loc[X_masked.index[mask_indices], col] = np.nan\n    \n    # Statistiques\n    total_values = X_clean.shape[0] * X_clean.shape[1]\n    masked_values = X_masked.isna().sum().sum()\n    print(f\"✓ Masqué {masked_values:,} valeurs sur {total_values:,} ({masked_values/total_values*100:.1f}%)\")\n    \n    return X_masked, Y_true\n\n\ndef pretrain_model(model, X_tr, clean_cols, config, n_epochs_pretrain=20):\n    \"\"\"\n    Pré-entraîner le modèle sur les courbes complètes\n    \n    Args:\n        model: BiLSTMImputer non entraîné\n        X_tr: DataFrame complet\n        clean_cols: colonnes des courbes complètes\n        config: Config object\n        n_epochs_pretrain: nombre d'epochs de pré-entraînement\n    \n    Returns:\n        model: modèle pré-entraîné\n        scaler: scaler utilisé\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"PHASE 1 : PRÉ-ENTRAÎNEMENT sur les 20k courbes complètes\")\n    print(\"=\"*60)\n    \n    # Créer données masquées\n    X_masked, Y_true = create_masked_data(\n        X_tr[clean_cols], \n        mask_prob=0.15,  # Masquer 15% des valeurs\n        seed=42\n    )\n    \n    # Split train/val (90/10 car on a beaucoup de données)\n    n_clean = len(clean_cols)\n    n_train = int(0.9 * n_clean)\n    \n    train_cols_pretrain = clean_cols[:n_train]\n    val_cols_pretrain = clean_cols[n_train:]\n    \n    print(f\"\\nPré-entraînement : {len(train_cols_pretrain)} train, {len(val_cols_pretrain)} val\")\n\n    train_dataset = TimeSeriesDataset(\n            X_masked[train_cols_pretrain],\n            Y_true[train_cols_pretrain],\n            fit_scaler=True\n        )\n        \n    val_dataset = TimeSeriesDataset(\n            X_masked[val_cols_pretrain],\n            Y_true[val_cols_pretrain],\n            scaler=train_dataset.scaler\n        )\n        \n    # DataLoaders\n    train_loader = DataLoader(\n            train_dataset,\n            batch_size=config.batch_size,\n            shuffle=True,\n            num_workers=2,\n            pin_memory=True\n        )\n        \n    val_loader = DataLoader(\n            val_dataset,\n            batch_size=config.batch_size,\n            shuffle=False,\n            num_workers=2,\n            pin_memory=True\n        )\n    print(f\"\\nDébut du pré-entraînement ({n_epochs_pretrain} epochs)...\")\n    \n    # Sauvegarder config original\n    original_epochs = config.num_epochs\n    config.num_epochs = n_epochs_pretrain\n    \n    model = train_model(model, train_loader, val_loader, config, train_dataset.scaler)\n    \n    # Restaurer config\n    config.num_epochs = original_epochs\n    \n    print(\"\\n✓ Pré-entraînement terminé !\")\n    print(\"Le modèle a appris les patterns généraux de consommation électrique\")\n    \n    return model, train_dataset.scaler\n\n\ndef finetune_model(model, scaler, X_tr, Y_tr, holed_cols, config):\n    \"\"\"\n    Fine-tuner le modèle pré-entraîné sur les vraies courbes à trous\n    \n    Args:\n        model: modèle pré-entraîné\n        scaler: scaler du pré-entraînement\n        X_tr: DataFrame avec courbes à trous\n        Y_tr: vraies valeurs\n        holed_cols: colonnes des courbes à trous\n        config: Config object\n    \n    Returns:\n        model: modèle fine-tuné\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"PHASE 2 : FINE-TUNING sur les 1000 courbes à trous réels\")\n    print(\"=\"*60)\n    \n    # Split train/val\n    n_holed = len(holed_cols)\n    n_train = int(0.8 * n_holed)\n    \n    train_cols = holed_cols[:n_train]\n    val_cols = holed_cols[n_train:]\n    \n    print(f\"Fine-tuning : {len(train_cols)} train, {len(val_cols)} val\")\n    \n    # Créer datasets (réutiliser le scaler du pré-entraînement!)\n    train_dataset = TimeSeriesDataset(\n        X_tr[train_cols],\n        Y_tr[train_cols],\n        scaler=scaler,\n        fit_scaler=False  # Important : ne pas refitter le scaler!\n    )\n    \n    val_dataset = TimeSeriesDataset(\n        X_tr[val_cols],\n        Y_tr[val_cols],\n        scaler=scaler,\n        fit_scaler=False\n    )\n    \n    # DataLoaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config.batch_size,\n        shuffle=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config.batch_size,\n        shuffle=False\n    )\n    \n    # Fine-tuning avec learning rate plus faible\n    print(f\"\\nDébut du fine-tuning avec LR réduit...\")\n\n    # Réduire le learning rate pour le fine-tuning\n    original_lr = config.learning_rate\n    config.learning_rate = original_lr * 0.1  # 10x plus petit\n    \n    model = train_model(model, train_loader, val_loader, config, scaler)\n    \n    # Restaurer learning rate\n    config.learning_rate = original_lr\n    \n    print(\"\\n✓ Fine-tuning terminé !\")\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:12:40.287546Z","iopub.execute_input":"2025-11-15T15:12:40.287805Z","iopub.status.idle":"2025-11-15T15:12:40.304942Z","shell.execute_reply.started":"2025-11-15T15:12:40.287753Z","shell.execute_reply":"2025-11-15T15:12:40.304247Z"}},"outputs":[],"execution_count":190},{"cell_type":"code","source":"def train_with_pretraining(X_tr, Y_tr, holed_cols, clean_cols, config, \n                           n_epochs_pretrain=20):\n    \"\"\"\n    Pipeline complet : pré-entraînement + fine-tuning\n    \n    Args:\n        X_tr: DataFrame avec toutes les courbes\n        Y_tr: vraies valeurs des courbes à trous\n        holed_cols: colonnes des courbes à trous\n        clean_cols: colonnes des courbes complètes\n        config: Config object\n        n_epochs_pretrain: epochs de pré-entraînement\n    \n    Returns:\n        model: modèle entraîné\n        scaler: scaler utilisé\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"ENTRAÎNEMENT AVEC PRÉ-ENTRAÎNEMENT\")\n    print(\"=\"*60)\n    print(f\"Courbes complètes (pré-entraînement) : {len(clean_cols)}\")\n    print(f\"Courbes à trous (fine-tuning) : {len(holed_cols)}\")\n    print(\"=\"*60)\n    \n    # Créer le modèle\n    model = BiLSTMImputer(\n        input_size=2,\n        hidden_size=config.hidden_size,\n        num_layers=config.num_layers,\n        dropout=config.dropout\n    ).to(config.device)\n    \n    print(f\"\\nModèle créé : {sum(p.numel() for p in model.parameters()):,} paramètres\")\n    \n    # PHASE 1 : Pré-entraînement\n    model, scaler = pretrain_model(\n        model, \n        X_tr, \n        clean_cols, \n        config,\n        n_epochs_pretrain=n_epochs_pretrain\n    )\n    \n    # PHASE 2 : Fine-tuning\n    model = finetune_model(\n        model,\n        scaler,\n        X_tr,\n        Y_tr,\n        holed_cols,\n        config\n    )\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"ENTRAÎNEMENT COMPLET TERMINÉ !\")\n    print(\"=\"*60)\n    \n    return model, scaler\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:12:40.305690Z","iopub.execute_input":"2025-11-15T15:12:40.306036Z","iopub.status.idle":"2025-11-15T15:12:40.323216Z","shell.execute_reply.started":"2025-11-15T15:12:40.306011Z","shell.execute_reply":"2025-11-15T15:12:40.322544Z"}},"outputs":[],"execution_count":191},{"cell_type":"markdown","source":"## Entraînement","metadata":{}},{"cell_type":"code","source":"config = Config()\n\n\n# Entraîner avec pré-entraînement\nmodel, scaler = train_with_pretraining(\n    X_tr=X_tr,\n    Y_tr=Y_tr,\n    holed_cols=holed_cols,\n    clean_cols=clean_cols,\n    config=config,\n    n_epochs_pretrain=15  # Ajuster selon le temps disponible\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:12:40.324006Z","iopub.execute_input":"2025-11-15T15:12:40.324501Z","iopub.status.idle":"2025-11-15T15:39:08.312897Z","shell.execute_reply.started":"2025-11-15T15:12:40.324478Z","shell.execute_reply":"2025-11-15T15:39:08.311833Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Training on 799 series, validating on 200 series\n\nModel architecture:\nBiLSTMImputer(\n  (bilstm): LSTM(2, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n  (fc): Linear(in_features=512, out_features=1, bias=True)\n)\n\nTotal parameters: 2,109,953\n\nStarting training...\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n  Train Loss: 0.6692\n  Val Loss: 0.4143, Val MAE: 184.99\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/50\n  Train Loss: 0.4406\n  Val Loss: 0.3670, Val MAE: 178.39\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/50\n  Train Loss: 0.4049\n  Val Loss: 0.3018, Val MAE: 181.16\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/50\n  Train Loss: 0.2950\n  Val Loss: 0.2609, Val MAE: 145.96\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/50\n  Train Loss: 0.2540\n  Val Loss: 0.2271, Val MAE: 153.20\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6/50\n  Train Loss: 0.2303\n  Val Loss: 0.2144, Val MAE: 126.35\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7/50\n  Train Loss: 0.2264\n  Val Loss: 0.2270, Val MAE: 129.71\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8/50\n  Train Loss: 0.2233\n  Val Loss: 0.1887, Val MAE: 114.97\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9/50\n  Train Loss: 0.2147\n  Val Loss: 0.1818, Val MAE: 119.82\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10/50\n  Train Loss: 0.2054\n  Val Loss: 0.1874, Val MAE: 125.07\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 11/50\n  Train Loss: 0.1998\n  Val Loss: 0.1851, Val MAE: 119.52\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 12/50\n  Train Loss: 0.2078\n  Val Loss: 0.1764, Val MAE: 108.04\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 13/50\n  Train Loss: 0.2042\n  Val Loss: 0.2437, Val MAE: 125.18\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 14/50\n  Train Loss: 0.2229\n  Val Loss: 0.1840, Val MAE: 118.64\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 15/50\n  Train Loss: 0.2044\n  Val Loss: 0.1737, Val MAE: 122.08\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 16/50\n  Train Loss: 0.2026\n  Val Loss: 0.1677, Val MAE: 107.77\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 17/50\n  Train Loss: 0.1976\n  Val Loss: 0.1833, Val MAE: 110.12\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 18/50\n  Train Loss: 0.1840\n  Val Loss: 0.1773, Val MAE: 108.04\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 19/50\n  Train Loss: 0.1825\n  Val Loss: 0.1650, Val MAE: 109.32\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 20/50\n  Train Loss: 0.1826\n  Val Loss: 0.1622, Val MAE: 106.37\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 21/50\n  Train Loss: 0.1791\n  Val Loss: 0.1672, Val MAE: 102.92\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 22/50\n  Train Loss: 0.1829\n  Val Loss: 0.1884, Val MAE: 115.01\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 23/50\n  Train Loss: 0.1895\n  Val Loss: 0.1585, Val MAE: 103.88\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 24/50\n  Train Loss: 0.1794\n  Val Loss: 0.1712, Val MAE: 105.10\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 25/50\n  Train Loss: 0.1834\n  Val Loss: 0.1572, Val MAE: 104.92\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 26/50\n  Train Loss: 0.1675\n  Val Loss: 0.1597, Val MAE: 101.39\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 27/50\n  Train Loss: 0.1733\n  Val Loss: 0.1726, Val MAE: 107.44\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 28/50\n  Train Loss: 0.1792\n  Val Loss: 0.1636, Val MAE: 108.96\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 29/50\n  Train Loss: 0.1699\n  Val Loss: 0.1561, Val MAE: 100.95\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 30/50\n  Train Loss: 0.1661\n  Val Loss: 0.1498, Val MAE: 98.32\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 31/50\n  Train Loss: 0.1669\n  Val Loss: 0.1471, Val MAE: 98.82\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 32/50\n  Train Loss: 0.1623\n  Val Loss: 0.1529, Val MAE: 105.82\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 33/50\n  Train Loss: 0.1651\n  Val Loss: 0.1488, Val MAE: 99.71\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 34/50\n  Train Loss: 0.1707\n  Val Loss: 0.1973, Val MAE: 118.10\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 35/50\n  Train Loss: 0.1868\n  Val Loss: 0.1638, Val MAE: 107.42\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 36/50\n  Train Loss: 0.1718\n  Val Loss: 0.1549, Val MAE: 104.80\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 37/50\n  Train Loss: 0.1740\n  Val Loss: 0.1476, Val MAE: 99.88\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 38/50\n  Train Loss: 0.1623\n  Val Loss: 0.1465, Val MAE: 96.16\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 39/50\n  Train Loss: 0.1549\n  Val Loss: 0.1478, Val MAE: 99.43\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 40/50\n  Train Loss: 0.1589\n  Val Loss: 0.1437, Val MAE: 94.04\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 41/50\n  Train Loss: 0.1606\n  Val Loss: 0.1417, Val MAE: 98.92\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 42/50\n  Train Loss: 0.1572\n  Val Loss: 0.1438, Val MAE: 93.70\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 43/50\n  Train Loss: 0.1542\n  Val Loss: 0.1402, Val MAE: 95.23\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 44/50\n  Train Loss: 0.1551\n  Val Loss: 0.1393, Val MAE: 93.58\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 45/50\n  Train Loss: 0.1546\n  Val Loss: 0.1408, Val MAE: 94.67\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 46/50\n  Train Loss: 0.1583\n  Val Loss: 0.1408, Val MAE: 96.28\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 47/50\n  Train Loss: 0.1530\n  Val Loss: 0.1406, Val MAE: 94.47\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 48/50\n  Train Loss: 0.1502\n  Val Loss: 0.1360, Val MAE: 91.11\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 49/50\n  Train Loss: 0.1505\n  Val Loss: 0.1528, Val MAE: 100.97\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 50/50\n  Train Loss: 0.1537\n  Val Loss: 0.1363, Val MAE: 93.50\n\n============================================================\nENTRAÎNEMENT AVEC PRÉ-ENTRAÎNEMENT\n============================================================\nCourbes complètes (pré-entraînement) : 20000\nCourbes à trous (fine-tuning) : 999\n============================================================\n\nModèle créé : 2,109,953 paramètres\n\n============================================================\nPHASE 1 : PRÉ-ENTRAÎNEMENT sur les 20k courbes complètes\n============================================================\nCréation de données masquées (mask_prob=0.15)...\n","output_type":"stream"},{"name":"stderr","text":"Masking: 100%|██████████| 20000/20000 [00:06<00:00, 2979.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"✓ Masqué 3,160,525 valeurs sur 21,140,000 (15.0%)\n\nPré-entraînement : 18000 train, 2000 val\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nDébut du pré-entraînement (15 epochs)...\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15\n  Train Loss: nan\n  Val Loss: nan, Val MAE: nan\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/15\n  Train Loss: nan\n  Val Loss: nan, Val MAE: nan\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/15\n  Train Loss: nan\n  Val Loss: nan, Val MAE: nan\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/15\n  Train Loss: nan\n  Val Loss: nan, Val MAE: nan\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/15\n  Train Loss: nan\n  Val Loss: nan, Val MAE: nan\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6/15\n  Train Loss: nan\n  Val Loss: nan, Val MAE: nan\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7/15\n  Train Loss: nan\n  Val Loss: nan, Val MAE: nan\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8/15\n  Train Loss: nan\n  Val Loss: nan, Val MAE: nan\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9/15\n  Train Loss: nan\n  Val Loss: nan, Val MAE: nan\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/15\n  Train Loss: nan\n  Val Loss: nan, Val MAE: nan\nEarly stopping at epoch 10\n\n✓ Pré-entraînement terminé !\nLe modèle a appris les patterns généraux de consommation électrique\n\n============================================================\nPHASE 2 : FINE-TUNING sur les 1000 courbes à trous réels\n============================================================\nFine-tuning : 799 train, 200 val\n\nDébut du fine-tuning avec LR réduit...\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n  Train Loss: 0.1611\n  Val Loss: 0.1470, Val MAE: 91.08\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/50\n  Train Loss: 0.1581\n  Val Loss: 0.1468, Val MAE: 91.06\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/50\n  Train Loss: 0.1601\n  Val Loss: 0.1461, Val MAE: 92.84\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/50\n  Train Loss: 0.1601\n  Val Loss: 0.1455, Val MAE: 91.67\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/50\n  Train Loss: 0.1604\n  Val Loss: 0.1452, Val MAE: 91.06\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6/50\n  Train Loss: 0.1589\n  Val Loss: 0.1462, Val MAE: 91.72\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7/50\n  Train Loss: 0.1590\n  Val Loss: 0.1462, Val MAE: 91.50\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8/50\n  Train Loss: 0.1572\n  Val Loss: 0.1479, Val MAE: 92.52\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9/50\n  Train Loss: 0.1555\n  Val Loss: 0.1444, Val MAE: 91.40\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10/50\n  Train Loss: 0.1560\n  Val Loss: 0.1442, Val MAE: 90.08\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 11/50\n  Train Loss: 0.1576\n  Val Loss: 0.1440, Val MAE: 90.51\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 12/50\n  Train Loss: 0.1587\n  Val Loss: 0.1444, Val MAE: 91.48\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 13/50\n  Train Loss: 0.1635\n  Val Loss: 0.1444, Val MAE: 90.94\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 14/50\n  Train Loss: 0.1586\n  Val Loss: 0.1440, Val MAE: 91.06\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 15/50\n  Train Loss: 0.1590\n  Val Loss: 0.1434, Val MAE: 90.60\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 16/50\n  Train Loss: 0.1608\n  Val Loss: 0.1444, Val MAE: 90.13\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 17/50\n  Train Loss: 0.1603\n  Val Loss: 0.1441, Val MAE: 90.38\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 18/50\n  Train Loss: 0.1556\n  Val Loss: 0.1446, Val MAE: 90.22\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 19/50\n  Train Loss: 0.1618\n  Val Loss: 0.1438, Val MAE: 90.32\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 20/50\n  Train Loss: 0.1596\n  Val Loss: 0.1447, Val MAE: 90.88\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 21/50\n  Train Loss: 0.1619\n  Val Loss: 0.1444, Val MAE: 93.64\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 22/50\n  Train Loss: 0.1582\n  Val Loss: 0.1431, Val MAE: 89.97\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 23/50\n  Train Loss: 0.1570\n  Val Loss: 0.1433, Val MAE: 90.29\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 24/50\n  Train Loss: 0.1576\n  Val Loss: 0.1442, Val MAE: 90.81\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 25/50\n  Train Loss: 0.1536\n  Val Loss: 0.1432, Val MAE: 89.55\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 26/50\n  Train Loss: 0.1516\n  Val Loss: 0.1431, Val MAE: 90.35\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 27/50\n  Train Loss: 0.1676\n  Val Loss: 0.1431, Val MAE: 90.61\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 28/50\n  Train Loss: 0.1548\n  Val Loss: 0.1430, Val MAE: 89.74\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 29/50\n  Train Loss: 0.1545\n  Val Loss: 0.1438, Val MAE: 90.56\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 30/50\n  Train Loss: 0.1551\n  Val Loss: 0.1436, Val MAE: 90.42\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 31/50\n  Train Loss: 0.1576\n  Val Loss: 0.1427, Val MAE: 90.53\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 32/50\n  Train Loss: 0.1545\n  Val Loss: 0.1430, Val MAE: 90.44\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 33/50\n  Train Loss: 0.1551\n  Val Loss: 0.1428, Val MAE: 89.86\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 34/50\n  Train Loss: 0.1532\n  Val Loss: 0.1424, Val MAE: 88.90\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 35/50\n  Train Loss: 0.1519\n  Val Loss: 0.1429, Val MAE: 89.41\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 36/50\n  Train Loss: 0.1545\n  Val Loss: 0.1437, Val MAE: 90.47\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 37/50\n  Train Loss: 0.1632\n  Val Loss: 0.1428, Val MAE: 90.09\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 38/50\n  Train Loss: 0.1524\n  Val Loss: 0.1425, Val MAE: 88.76\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 39/50\n  Train Loss: 0.1570\n  Val Loss: 0.1422, Val MAE: 89.38\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 40/50\n  Train Loss: 0.1564\n  Val Loss: 0.1431, Val MAE: 89.73\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 41/50\n  Train Loss: 0.1529\n  Val Loss: 0.1422, Val MAE: 89.45\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 42/50\n  Train Loss: 0.1538\n  Val Loss: 0.1423, Val MAE: 89.83\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 43/50\n  Train Loss: 0.1560\n  Val Loss: 0.1427, Val MAE: 89.67\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 44/50\n  Train Loss: 0.1582\n  Val Loss: 0.1427, Val MAE: 90.07\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 45/50\n  Train Loss: 0.1549\n  Val Loss: 0.1426, Val MAE: 89.61\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 46/50\n  Train Loss: 0.1525\n  Val Loss: 0.1423, Val MAE: 89.25\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 47/50\n  Train Loss: 0.1543\n  Val Loss: 0.1421, Val MAE: 90.03\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 48/50\n  Train Loss: 0.1568\n  Val Loss: 0.1420, Val MAE: 89.67\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 49/50\n  Train Loss: 0.1543\n  Val Loss: 0.1424, Val MAE: 89.31\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 50/50\n  Train Loss: 0.1568\n  Val Loss: 0.1422, Val MAE: 89.67\n\n✓ Fine-tuning terminé !\n\n============================================================\nENTRAÎNEMENT COMPLET TERMINÉ !\n============================================================\n","output_type":"stream"}],"execution_count":192},{"cell_type":"markdown","source":"## Prédiction sur le X_test","metadata":{}},{"cell_type":"code","source":"X_test_holed_cols = [c for c in X_test.columns if c.startswith(\"holed\")]\nX_test_imputed = predict(model, X_test[X_test_holed_cols], scaler, config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test_imputed","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test_imputed.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T15:39:10.865097Z","iopub.execute_input":"2025-11-15T15:39:10.865426Z","iopub.status.idle":"2025-11-15T15:39:10.881791Z","shell.execute_reply.started":"2025-11-15T15:39:10.865395Z","shell.execute_reply":"2025-11-15T15:39:10.881088Z"}},"outputs":[{"execution_count":196,"output_type":"execute_result","data":{"text/plain":"(1057, 1000)"},"metadata":{}}],"execution_count":196},{"cell_type":"code","source":"pd.DataFrame(X_test_imputed, \n             index=X_test.index, \n             columns=X_test_holed_cols).to_csv(\"predictions_bilstm.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}