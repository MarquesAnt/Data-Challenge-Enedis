{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13741897,"sourceType":"datasetVersion","datasetId":8743715}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm\nfrom pathlib import Path","metadata":{"_uuid":"8c9b87fb-2cb5-47df-bc5f-c9ec3740f4bf","_cell_guid":"c5009ce4-a65b-46a1-81ad-564d76346a95","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-20T10:11:00.948045Z","iopub.execute_input":"2025-11-20T10:11:00.948236Z","iopub.status.idle":"2025-11-20T10:11:10.373561Z","shell.execute_reply.started":"2025-11-20T10:11:00.948220Z","shell.execute_reply":"2025-11-20T10:11:10.372946Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Importation des données d'entraînement et de test","metadata":{"_uuid":"424a2f10-0752-4232-8a5e-45e55932bfec","_cell_guid":"8e96c7da-05d7-4292-8ec1-26ef5d4817de","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"DATA_PATH = Path(\"../input/preprocessed\")\nX_tr = pd.read_csv(DATA_PATH / \"X_train.csv\", index_col=0,parse_dates=True)\nX_test = pd.read_csv(DATA_PATH / \"X_test.csv\", index_col=0,parse_dates=True)\nY_tr = pd.read_csv(DATA_PATH / \"y_train.csv\", index_col=0,parse_dates=True)","metadata":{"_uuid":"1c84e957-5131-45a2-8bb6-5c18847a318d","_cell_guid":"b42a8fa9-05f5-4bce-b859-230c78fba15a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-20T10:11:10.375120Z","iopub.execute_input":"2025-11-20T10:11:10.375445Z","iopub.status.idle":"2025-11-20T10:12:05.780082Z","shell.execute_reply.started":"2025-11-20T10:11:10.375427Z","shell.execute_reply":"2025-11-20T10:12:05.779411Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"X_tr = np.expm1(X_tr)\nX_test = np.expm1(X_test)\nY_tr = np.expm1(Y_tr)","metadata":{"_uuid":"edfee10e-8d0a-42c3-b7e8-32eb1d1d657a","_cell_guid":"9c4411be-8ff3-4e01-85f2-db8c177565e7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-20T10:12:05.780877Z","iopub.execute_input":"2025-11-20T10:12:05.781083Z","iopub.status.idle":"2025-11-20T10:12:05.913908Z","shell.execute_reply.started":"2025-11-20T10:12:05.781066Z","shell.execute_reply":"2025-11-20T10:12:05.912436Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: invalid value encountered in expm1\n  result = func(self.values, **kwargs)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"holed_cols = [c for c in X_tr.columns if c.startswith(\"holed\")]\nholed_cols_t = [c for c in X_test.columns if c.startswith(\"holed\")]\nclean_cols = [c for c in X_tr.columns if c not in holed_cols]\nclean_cols_t = [c for c in X_test.columns if c not in holed_cols_t]","metadata":{"_uuid":"43838757-dd66-4284-89ac-5b3e220ed061","_cell_guid":"94ffb997-1e5c-4cdd-8155-549be733c12c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-20T10:12:05.914648Z","iopub.execute_input":"2025-11-20T10:12:05.915377Z","iopub.status.idle":"2025-11-20T10:12:06.562189Z","shell.execute_reply.started":"2025-11-20T10:12:05.915343Z","shell.execute_reply":"2025-11-20T10:12:06.561535Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"assert list(holed_cols) == list(Y_tr.columns)","metadata":{"_uuid":"81fd65cb-04eb-42b7-9e8c-6c7d2d097ec3","_cell_guid":"cb80d36d-e76b-473c-8cce-a7b1279aedb5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-20T10:12:06.562996Z","iopub.execute_input":"2025-11-20T10:12:06.563258Z","iopub.status.idle":"2025-11-20T10:12:06.567203Z","shell.execute_reply.started":"2025-11-20T10:12:06.563239Z","shell.execute_reply":"2025-11-20T10:12:06.566384Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"X_all = pd.concat([\n    X_tr,      # 20k courbes\n    X_test[clean_cols_t]  # 40k courbes\n], axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:12:06.568073Z","iopub.execute_input":"2025-11-20T10:12:06.568373Z","iopub.status.idle":"2025-11-20T10:12:06.865753Z","shell.execute_reply.started":"2025-11-20T10:12:06.568356Z","shell.execute_reply":"2025-11-20T10:12:06.864920Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"holed_cols = [c for c in X_all.columns if c.startswith(\"holed\")]\nclean_cols = [c for c in X_all.columns if c not in holed_cols]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:12:06.868521Z","iopub.execute_input":"2025-11-20T10:12:06.868769Z","iopub.status.idle":"2025-11-20T10:12:07.501616Z","shell.execute_reply.started":"2025-11-20T10:12:06.868750Z","shell.execute_reply":"2025-11-20T10:12:07.500682Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Interpolation Linéaire : benchmark","metadata":{}},{"cell_type":"code","source":"# ============= Interpolation Linéaire (Benchmark) =============\n\ndef fill_nan_with_interpolation(column):\n    \"\"\"\n    Méthode du benchmark : interpolation linéaire\n    MAE = 104\n    \"\"\"\n    col = column.copy()\n    col = col.interpolate(method='linear', limit_direction='both')\n    return col\n\n\ndef predict_interpolation(X_test_holed):\n    \"\"\"\n    Prédire avec interpolation linéaire (méthode benchmark)\n    \n    Args:\n        X_test_holed: DataFrame avec les colonnes à trous de X_test\n    \n    Returns:\n        y_pred: DataFrame avec valeurs imputées\n    \"\"\"\n    print(\"Prédiction par interpolation linéaire...\")\n    y_pred = X_test_holed.apply(fill_nan_with_interpolation, axis=0)\n    \n    # Vérifier qu'il n'y a plus de NaN\n    remaining_nan = y_pred.isna().sum().sum()\n    if remaining_nan > 0:\n        print(f\" Attention : {remaining_nan} NaN restants après interpolation\")\n        # Fallback : forward fill puis backward fill\n        y_pred = y_pred.fillna(method='ffill').fillna(method='bfill')\n    \n    print(f\"✓ Interpolation terminée : {y_pred.shape[1]} courbes imputées\")\n    return y_pred\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T10:12:07.502497Z","iopub.execute_input":"2025-11-20T10:12:07.503038Z","iopub.status.idle":"2025-11-20T10:12:07.508582Z","shell.execute_reply.started":"2025-11-20T10:12:07.503009Z","shell.execute_reply":"2025-11-20T10:12:07.507906Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Config","metadata":{"_uuid":"2c332e12-00d4-423e-a647-56ca709337a4","_cell_guid":"781c58fd-f826-4599-8de2-de377569f9fc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class Config:\n    hidden_size = 256\n    num_layers = 3\n    dropout = 0.3\n    batch_size = 32\n    learning_rate = 0.0005\n    num_epochs = 50\n    patience = 10\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nconfig = Config()\nprint(f\"Using device: {config.device}\")","metadata":{"_uuid":"439a69e0-d7e0-46b7-8446-10a44c2bf674","_cell_guid":"f58bfbe9-7161-488a-aef9-c2589488ccb0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-20T10:12:07.509501Z","iopub.execute_input":"2025-11-20T10:12:07.510004Z","iopub.status.idle":"2025-11-20T10:12:07.643192Z","shell.execute_reply.started":"2025-11-20T10:12:07.509979Z","shell.execute_reply":"2025-11-20T10:12:07.642301Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Classes","metadata":{"_uuid":"1a96502f-fe13-41c1-b814-789e9a9fe750","_cell_guid":"26d275b7-1fd2-4327-9eb0-e0c5eb5d386c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Fonctions d'entraînement","metadata":{"_uuid":"8ebe693a-0d34-4bf6-960a-29767440dc2b","_cell_guid":"d415c747-376d-4a0e-98f5-64bd84879166","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ============= Dataset =============\n\nclass TimeSeriesDataset(Dataset):\n    \"\"\"Dataset pour les séries temporelles avec valeurs manquantes\"\"\"\n    \n    def __init__(self, X, y=None, scaler=None, fit_scaler=False):\n        \"\"\"\n        X: DataFrame (timestamps × courbes)\n        y: DataFrame avec les vraies valeurs (même format que X)\n        \"\"\"\n        self.X = X.values.T  # Shape: (n_series, n_timesteps)\n        self.y = y.values.T if y is not None else None\n        \n        # Normalisation\n        if fit_scaler:\n            self.scaler = StandardScaler()\n            # Fit sur les valeurs non-NaN uniquement\n            valid_values = self.X[~np.isnan(self.X)].reshape(-1, 1)\n            self.scaler.fit(valid_values)\n        else:\n            self.scaler = scaler\n            \n        # Normaliser X\n        self.X_scaled = self.X.copy()\n        mask = ~np.isnan(self.X)\n        self.X_scaled[mask] = self.scaler.transform(self.X[mask].reshape(-1, 1)).flatten()\n        \n        # Remplacer les NaN par 0 pour le modèle (on utilisera le mask)\n        self.X_scaled = np.nan_to_num(self.X_scaled, nan=0.0)\n        \n        # Normaliser Y aussi !\n        if self.y is not None:\n            self.y_scaled = self.scaler.transform(self.y.reshape(-1, 1)).reshape(self.y.shape)\n        else:\n            self.y_scaled = None\n        \n        # Créer le masque (1 = valeur observée, 0 = manquante)\n        self.mask = (~np.isnan(self.X)).astype(np.float32)\n\n    def __len__(self):\n        return self.X.shape[0]  # Nombre de séries\n    \n    def __getitem__(self, idx):\n        x = torch.FloatTensor(self.X_scaled[idx]).unsqueeze(-1)  # (timesteps, 1)\n        mask = torch.FloatTensor(self.mask[idx]).unsqueeze(-1)   # (timesteps, 1)\n        \n        if self.y_scaled is not None:\n            y = torch.FloatTensor(self.y_scaled[idx]).unsqueeze(-1)\n            return x, mask, y\n        return x, mask\n\n# ============= Modèle BiLSTM =============\n\nclass BiLSTMImputer(nn.Module):\n    \"\"\"Modèle BiLSTM bidirectionnel pour imputation\"\"\"\n    \n    def __init__(self, input_size=1, hidden_size=128, num_layers=2, dropout=0.2):\n        super().__init__()\n        \n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # BiLSTM\n        self.bilstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            dropout=dropout if num_layers > 1 else 0,\n            bidirectional=True,\n            batch_first=True\n        )\n        \n        # Couche de sortie\n        self.fc = nn.Linear(hidden_size * 2, 1)  # *2 car bidirectionnel\n        \n    def forward(self, x, mask):\n        \"\"\"\n        x: (batch, timesteps, 1)\n        mask: (batch, timesteps, 1)\n        \"\"\"\n        # Concaténer x et mask comme input\n        x_masked = torch.cat([x, mask], dim=-1)  # (batch, timesteps, 2)\n        \n        # Passer par le BiLSTM\n        lstm_out, _ = self.bilstm(x_masked)  # (batch, timesteps, hidden*2)\n        \n        # Prédiction\n        output = self.fc(lstm_out)  # (batch, timesteps, 1)\n        \n        return output\n\n\n# ============= Loss personnalisée =============\n\nclass MaskedMSELoss(nn.Module):\n    \"\"\"MSE loss qui ne calcule l'erreur que sur les valeurs manquantes\"\"\"\n    \n    def forward(self, pred, target, mask):\n        \"\"\"\n        pred: prédictions (batch, timesteps, 1)\n        target: vraies valeurs (batch, timesteps, 1)\n        mask: masque (batch, timesteps, 1) - 1=observé, 0=manquant\n        \"\"\"\n        # On veut prédire uniquement les valeurs manquantes\n        missing_mask = (1 - mask)  # Inverser le masque\n        \n        # Calculer l'erreur uniquement sur les valeurs manquantes\n        error = (pred - target) ** 2\n        masked_error = error * missing_mask\n        \n        # Moyenne sur les valeurs manquantes uniquement\n        n_missing = missing_mask.sum()\n        if n_missing > 0:\n            loss = masked_error.sum() / n_missing\n        else:\n            loss = torch.tensor(0.0, device=pred.device)\n        \n        return loss","metadata":{"_uuid":"7162a836-046d-4e4a-b8ff-32918ccc88d6","_cell_guid":"e0a3e903-a804-4d83-8e81-1645bc5743c0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-20T10:12:07.644154Z","iopub.execute_input":"2025-11-20T10:12:07.644437Z","iopub.status.idle":"2025-11-20T10:12:07.670316Z","shell.execute_reply.started":"2025-11-20T10:12:07.644410Z","shell.execute_reply":"2025-11-20T10:12:07.669678Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    \n    for x, mask, y in tqdm(loader, desc=\"Training\", leave=False):\n        x, mask, y = x.to(device), mask.to(device), y.to(device)\n        \n        optimizer.zero_grad()\n        \n        # Forward\n        pred = model(x, mask)\n        loss = criterion(pred, y, mask)\n        \n        # Backward\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(loader)\n\ndef validate(model, loader, criterion, device, scaler):\n    model.eval()\n    total_loss = 0\n    total_mae = 0\n    \n    with torch.no_grad():\n        for x, mask, y in loader:\n            x, mask, y = x.to(device), mask.to(device), y.to(device)\n            \n            pred = model(x, mask)\n            loss = criterion(pred, y, mask)\n            \n            # Calculer MAE sur les vraies valeurs (dénormalisées)\n            pred_denorm = scaler.inverse_transform(pred.cpu().numpy().reshape(-1, 1))\n            y_denorm = scaler.inverse_transform(y.cpu().numpy().reshape(-1, 1))\n            mask_cpu = mask.cpu().numpy().reshape(-1, 1)\n            \n            # MAE seulement sur les valeurs manquantes\n            missing_mask = (1 - mask_cpu).astype(bool).flatten()\n            if missing_mask.sum() > 0:\n                mae = np.abs(pred_denorm.flatten()[missing_mask] - \n                           y_denorm.flatten()[missing_mask]).mean()\n                total_mae += mae\n            \n            total_loss += loss.item()\n    \n    return total_loss / len(loader), total_mae / len(loader)\n\ndef train_model(model, train_loader, val_loader, config, scaler):\n    \"\"\"Entraînement avec early stopping\"\"\"\n    \n    criterion = MaskedMSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n    )\n    \n    best_val_loss = float('inf')\n    patience_counter = 0\n    \n    for epoch in range(config.num_epochs):\n        # Train\n        train_loss = train_epoch(model, train_loader, optimizer, criterion, config.device)\n        \n        # Validate\n        val_loss, val_mae = validate(model, val_loader, criterion, config.device, scaler)\n        \n        # Scheduler\n        scheduler.step(val_loss)\n        \n        print(f\"Epoch {epoch+1}/{config.num_epochs}\")\n        print(f\"  Train Loss: {train_loss:.4f}\")\n        print(f\"  Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.2f}\")\n        \n        # Early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            # Sauvegarder le meilleur modèle\n            torch.save(model.state_dict(), 'best_bilstm_model.pt')\n            print(\"  → New best model saved!\")\n        else:\n            patience_counter += 1\n            \n        if patience_counter >= config.patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n    \n    # Charger le meilleur modèle\n    model.load_state_dict(torch.load('best_bilstm_model.pt'))\n    return model","metadata":{"_uuid":"19dc0bf8-0f53-4155-bc3f-ce6e489eb77a","_cell_guid":"57e392dc-2e3e-4047-b333-4b185f0b4933","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-20T10:12:07.671060Z","iopub.execute_input":"2025-11-20T10:12:07.671321Z","iopub.status.idle":"2025-11-20T10:12:07.690842Z","shell.execute_reply.started":"2025-11-20T10:12:07.671290Z","shell.execute_reply":"2025-11-20T10:12:07.689997Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Fonctions de prédictions","metadata":{"_uuid":"504e6f8a-8f4b-425a-8898-c9fdb4504541","_cell_guid":"2a6c677c-3053-4f68-a848-ab0c0f8f7076","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def predict(model, X_df, scaler, config, batch_size=64):\n    \"\"\"Prédire les valeurs manquantes\"\"\"\n    \n    # Créer dataset\n    dataset = TimeSeriesDataset(X_df, scaler=scaler, fit_scaler=False)\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    \n    model.eval()\n    all_predictions = []\n    \n    with torch.no_grad():\n        for x, mask in tqdm(loader, desc=\"Predicting\"):\n            x, mask = x.to(config.device), mask.to(config.device)\n            \n            pred = model(x, mask)\n            all_predictions.append(pred.cpu().numpy())\n    \n    # Concatener et reshape\n    predictions = np.concatenate(all_predictions, axis=0)  # (n_series, timesteps, 1)\n    predictions = predictions.squeeze(-1).T  # (timesteps, n_series)\n    \n    # Dénormaliser\n    predictions_denorm = scaler.inverse_transform(predictions.reshape(-1, 1))\n    predictions_denorm = predictions_denorm.reshape(predictions.shape)\n    \n    # Créer DataFrame\n    pred_df = pd.DataFrame(\n        predictions_denorm,\n        index=X_df.index,\n        columns=X_df.columns\n    )\n    \n    # Remplir uniquement les valeurs manquantes\n    result = X_df.copy()\n    mask = X_df.isna()\n    result[mask] = pred_df[mask]\n    \n    return result\n\n# ============= Fonction principale =============\ndef run_bilstm_imputation(X_tr, Y_tr, holed_cols, clean_cols, config):\n    \"\"\"\n    Pipeline complet d'entraînement et prédiction\n    \n    Returns:\n        model: modèle entraîné\n        scaler: scaler utilisé\n        train_loader: pour analyse\n        val_loader: pour analyse\n    \"\"\"\n    \n    # Séparer train/val (80/20 sur les courbes holed)\n    n_holed = len(holed_cols)\n    n_train = int(0.8 * n_holed)\n    \n    train_cols = holed_cols[:n_train]\n    val_cols = holed_cols[n_train:]\n    \n    print(f\"Training on {len(train_cols)} series, validating on {len(val_cols)} series\")\n    \n    # Créer datasets\n    train_dataset = TimeSeriesDataset(\n        X_tr[train_cols], \n        Y_tr[train_cols], \n        fit_scaler=True\n    )\n    \n    val_dataset = TimeSeriesDataset(\n        X_tr[val_cols], \n        Y_tr[val_cols], \n        scaler=train_dataset.scaler\n    )\n    \n    # DataLoaders\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=config.batch_size, \n        shuffle=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=config.batch_size, \n        shuffle=False\n    )\n    \n    # Créer modèle\n    model = BiLSTMImputer(\n        input_size=2,  # valeur + mask\n        hidden_size=config.hidden_size,\n        num_layers=config.num_layers,\n        dropout=config.dropout\n    ).to(config.device)\n    \n    print(f\"\\nModel architecture:\")\n    print(model)\n    print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    \n    # Entraîner\n    print(\"\\nStarting training...\")\n    model = train_model(model, train_loader, val_loader, config, train_dataset.scaler)\n    \n    return model, train_dataset.scaler, train_loader, val_loader","metadata":{"_uuid":"66e611f7-caaa-49ae-bf1f-a44c2f0c833c","_cell_guid":"cece3c27-0130-41c0-8f19-3715c612d1b7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-20T10:12:07.691813Z","iopub.execute_input":"2025-11-20T10:12:07.692092Z","iopub.status.idle":"2025-11-20T10:12:07.716073Z","shell.execute_reply.started":"2025-11-20T10:12:07.692039Z","shell.execute_reply":"2025-11-20T10:12:07.715246Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Pré-entraînement","metadata":{"_uuid":"cbd61ee9-287b-412b-93e3-c4ff8199a25d","_cell_guid":"641156c5-04d1-4af3-88d4-89c82aac0ec2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ============= PRÉ-ENTRAÎNEMENT =============\n\ndef analyze_missing_patterns(X_holed):\n    \"\"\"\n    Analyser la distribution des trous dans les données réelles\n    \n    Returns:\n        hole_size_distribution: distribution des tailles de trous\n        hole_rate_distribution: distribution des taux de NA par courbe\n    \"\"\"\n    hole_sizes = []\n    hole_rates = []\n    \n    for col in X_holed.columns:\n        series = X_holed[col]\n        is_na = series.isna()\n        \n        # Taux de NA pour cette courbe\n        na_rate = is_na.mean()\n        hole_rates.append(na_rate)\n        \n        # Trouver les tailles de blocs consécutifs de NaN\n        in_hole = False\n        current_hole_size = 0\n        \n        for val in is_na:\n            if val:  # NaN\n                if not in_hole:\n                    in_hole = True\n                    current_hole_size = 1\n                else:\n                    current_hole_size += 1\n            else:  # Pas NaN\n                if in_hole:\n                    hole_sizes.append(current_hole_size)\n                    in_hole = False\n                    current_hole_size = 0\n        \n        # Si la série se termine par un trou\n        if in_hole:\n            hole_sizes.append(current_hole_size)\n    \n    return np.array(hole_sizes), np.array(hole_rates)\n\n\ndef create_masked_data_realistic(X_clean, X_holed_reference, seed=42, oversample_large=True):\n    \"\"\"\n    Créer des données masquées en imitant la distribution réelle des trous\n    avec sur-échantillonnage optionnel des gros trous\n    \n    Args:\n        X_clean: DataFrame avec courbes complètes (pour pré-entraînement)\n        X_holed_reference: DataFrame avec vraies courbes à trous (pour analyser les patterns)\n        seed: graine aléatoire\n        oversample_large: si True, sur-échantillonner les gros trous (>30 timesteps)\n    \n    Returns:\n        X_masked: DataFrame avec masquage réaliste\n        Y_true: DataFrame avec vraies valeurs\n    \"\"\"\n    np.random.seed(seed)\n\n    # ----------------------------------------------------\n    # 1. ANALYSE DES DONNÉES RÉELLES\n    # ----------------------------------------------------\n    hole_sizes, hole_rates = analyze_missing_patterns(X_holed_reference)\n\n    # Découpage comme dans ta version\n    small_holes = hole_sizes[hole_sizes <= 12]\n    medium_holes = hole_sizes[(hole_sizes > 12) & (hole_sizes < 48)]\n    large_holes = hole_sizes[hole_sizes >= 48]\n\n    if oversample_large and len(large_holes) > 0:\n        large_holes_repeated = np.repeat(large_holes, 5)\n        hole_sizes_augmented = np.concatenate([hole_sizes, large_holes_repeated])\n    else:\n        hole_sizes_augmented = hole_sizes\n\n    # Nettoyage X_clean si besoin\n    if X_clean.isna().any().any():\n        X_clean = (X_clean.interpolate(method='linear', axis=0)\n                           .fillna(method='bfill')\n                           .fillna(method='ffill'))\n\n    X_masked = X_clean.copy()\n    Y_true = X_clean.copy()\n\n    n_rows = X_clean.shape[0]\n    n_cols = X_clean.shape[1]\n    columns = X_clean.columns\n\n    created_hole_sizes = []\n\n    print(\"\\nCréation des masques réalistes (version optimisée)...\")\n\n    # ----------------------------------------------------\n    # 2. BOUCLE SUR LES COLONNES (OPTIMISÉE)\n    # ----------------------------------------------------\n    for col in tqdm(columns, desc=\"Masking\"):\n        \n        # NumPy array pour vitesse\n        col_values = X_masked[col].values.astype(float)\n        \n        # Tirer un taux de NA réaliste\n        target_na_rate = np.random.choice(hole_rates)\n        target_na_count = int(n_rows * target_na_rate)\n\n        if target_na_count == 0:\n            continue\n        \n        # Masque booléen (beaucoup plus rapide que Pandas)\n        mask = np.zeros(n_rows, dtype=bool)\n        masked_count = 0\n        attempts = 0\n        max_attempts = 1000\n\n        # ------------------------------------------------\n        # Génération optimisée des blocs\n        # ------------------------------------------------\n        while masked_count < target_na_count and attempts < max_attempts:\n            attempts += 1\n\n            # Tirer une taille de trou depuis la distribution réaliste\n            hole_size = int(np.random.choice(hole_sizes_augmented))\n\n            # Limiter si nécessaire\n            remaining = target_na_count - masked_count\n            if hole_size > remaining:\n                hole_size = remaining\n            if hole_size <= 0:\n                break\n            \n            # Tirer un début possible\n            start = np.random.randint(0, n_rows - hole_size)\n            end = start + hole_size\n\n            # Vérifier overlap avec NumPy (ultra rapide)\n            if not mask[start:end].any():\n                mask[start:end] = True\n                masked_count += hole_size\n                created_hole_sizes.append(hole_size)\n\n        # Appliquer le masque en une seule opération NumPy\n        col_values[mask] = np.nan\n        X_masked[col] = col_values\n\n    # ----------------------------------------------------\n    # 3. STATISTIQUES FINALES\n    # ----------------------------------------------------\n    created_hole_sizes = np.array(created_hole_sizes)\n\n    total_values = X_clean.size\n    masked_values = X_masked.isna().sum().sum()\n\n    print(f\"\\n✓ Masqué {masked_values:,} valeurs sur {total_values:,} \"\n          f\"({masked_values/total_values*100:.1f}%)\")\n\n    if len(created_hole_sizes) > 0:\n        print(f\"✓ Distribution créée :\")\n        print(f\"  - Petits trous (1-12) : \"\n              f\"{np.sum(created_hole_sizes <= 12)}\")\n        print(f\"  - Moyens (13-47) : \"\n              f\"{np.sum((created_hole_sizes > 12) & (created_hole_sizes < 48))}\")\n        print(f\"  - Gros (48+) : \"\n              f\"{np.sum(created_hole_sizes >= 48)}\")\n    \n    return X_masked, Y_true\n\n\ndef pretrain_model(model, X_tr, clean_cols, holed_cols, config, n_epochs_pretrain=20):\n    \"\"\"\n    Pré-entraîner le modèle sur les courbes complètes\n    \n    Args:\n        model: BiLSTMImputer non entraîné\n        X_tr: DataFrame complet\n        clean_cols: colonnes des courbes complètes\n        holed_cols: colonnes des courbes à trous (pour analyser les patterns)\n        config: Config object\n        n_epochs_pretrain: nombre d'epochs de pré-entraînement\n    \n    Returns:\n        model: modèle pré-entraîné\n        scaler: scaler utilisé\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"PHASE 1 : PRÉ-ENTRAÎNEMENT sur les 20k courbes complètes\")\n    print(\"=\"*60)\n    \n    # Créer données masquées avec distribution réaliste\n    X_masked, Y_true = create_masked_data_realistic(\n        X_clean=X_tr[clean_cols],\n        X_holed_reference=X_tr[holed_cols],  # Analyser les vrais trous\n        seed=42\n    )\n    \n    # Split train/val (90/10 car on a beaucoup de données)\n    n_clean = len(clean_cols)\n    n_train = int(0.9 * n_clean)\n    \n    train_cols_pretrain = clean_cols[:n_train]\n    val_cols_pretrain = clean_cols[n_train:]\n    \n    print(f\"\\nPré-entraînement : {len(train_cols_pretrain)} train, {len(val_cols_pretrain)} val\")\n    \n    # Créer datasets    \n    train_dataset = TimeSeriesDataset(\n        X_masked[train_cols_pretrain],\n        Y_true[train_cols_pretrain],\n        fit_scaler=True\n    )\n    \n    val_dataset = TimeSeriesDataset(\n        X_masked[val_cols_pretrain],\n        Y_true[val_cols_pretrain],\n        scaler=train_dataset.scaler\n    )\n    \n    # DataLoaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config.batch_size,\n        shuffle=True,\n        num_workers=0,  # Changé de 2 à 0 pour Colab\n        pin_memory=True if torch.cuda.is_available() else False\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config.batch_size,\n        shuffle=False,\n        num_workers=0,  # Changé de 2 à 0 pour Colab\n        pin_memory=True if torch.cuda.is_available() else False\n    )\n    \n    # Entraîner    \n    print(f\"\\nDébut du pré-entraînement ({n_epochs_pretrain} epochs)...\")\n    \n    # Sauvegarder config original et réduire le LR pour stabilité\n    original_epochs = config.num_epochs\n    original_lr = config.learning_rate\n    config.num_epochs = n_epochs_pretrain\n    config.learning_rate = 0.0001  # LR plus faible pour le pré-entraînement\n    \n    print(f\"Learning rate réduit à {config.learning_rate} pour stabilité\")\n    \n    model = train_model(model, train_loader, val_loader, config, train_dataset.scaler)\n    \n    # Restaurer config\n    config.num_epochs = original_epochs\n    config.learning_rate = original_lr\n    \n    print(\"\\n✓ Pré-entraînement terminé !\")\n    print(\"Le modèle a appris les patterns généraux de consommation électrique\")\n    \n    return model, train_dataset.scaler\n\n\ndef finetune_model(model, scaler, X_tr, Y_tr, holed_cols, config):\n    \"\"\"\n    Fine-tuner le modèle pré-entraîné sur les vraies courbes à trous\n    \n    Args:\n        model: modèle pré-entraîné\n        scaler: scaler du pré-entraînement\n        X_tr: DataFrame avec courbes à trous\n        Y_tr: vraies valeurs\n        holed_cols: colonnes des courbes à trous\n        config: Config object\n    \n    Returns:\n        model: modèle fine-tuné\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"PHASE 2 : FINE-TUNING sur les 1000 courbes à trous réels\")\n    print(\"=\"*60)\n    \n    # Split train/val\n    n_holed = len(holed_cols)\n    n_train = int(0.8 * n_holed)\n    \n    train_cols = holed_cols[:n_train]\n    val_cols = holed_cols[n_train:]\n    \n    print(f\"Fine-tuning : {len(train_cols)} train, {len(val_cols)} val\")\n    \n    # Créer datasets (réutiliser le scaler du pré-entraînement!)\n    train_dataset = TimeSeriesDataset(\n        X_tr[train_cols],\n        Y_tr[train_cols],\n        scaler=scaler,\n        fit_scaler=False  # Important : ne pas refitter le scaler!\n    )\n    \n    val_dataset = TimeSeriesDataset(\n        X_tr[val_cols],\n        Y_tr[val_cols],\n        scaler=scaler,\n        fit_scaler=False\n    )\n    \n    # DataLoaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config.batch_size,\n        shuffle=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config.batch_size,\n        shuffle=False\n    )\n    \n    # Fine-tuning avec learning rate plus faible\n    print(f\"\\nDébut du fine-tuning avec LR réduit...\")\n\n    # Réduire le learning rate pour le fine-tuning\n    original_lr = config.learning_rate\n    config.learning_rate = original_lr * 0.1  # 10x plus petit\n    \n    model = train_model(model, train_loader, val_loader, config, scaler)\n    \n    # Restaurer learning rate\n    config.learning_rate = original_lr\n    \n    print(\"\\n✓ Fine-tuning terminé !\")\n    \n    return model","metadata":{"_uuid":"2d699814-bed2-478d-b93a-78a87b6f8d01","_cell_guid":"3ba83126-9f44-425d-aa9f-34050fb4d8b1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-20T10:12:07.717066Z","iopub.execute_input":"2025-11-20T10:12:07.717340Z","iopub.status.idle":"2025-11-20T10:12:07.747502Z","shell.execute_reply.started":"2025-11-20T10:12:07.717317Z","shell.execute_reply":"2025-11-20T10:12:07.746616Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def train_with_pretraining(X_tr, Y_tr, holed_cols, clean_cols, config, \n                           n_epochs_pretrain=20):\n    \"\"\"\n    Pipeline complet : pré-entraînement + fine-tuning\n    \n    Args:\n        X_tr: DataFrame avec toutes les courbes\n        Y_tr: vraies valeurs des courbes à trous\n        holed_cols: colonnes des courbes à trous\n        clean_cols: colonnes des courbes complètes\n        config: Config object\n        n_epochs_pretrain: epochs de pré-entraînement\n    \n    Returns:\n        model: modèle entraîné\n        scaler: scaler utilisé\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"ENTRAÎNEMENT AVEC PRÉ-ENTRAÎNEMENT\")\n    print(\"=\"*60)\n    print(f\"Courbes complètes (pré-entraînement) : {len(clean_cols)}\")\n    print(f\"Courbes à trous (fine-tuning) : {len(holed_cols)}\")\n    print(\"=\"*60)\n    \n    # Créer le modèle\n    model = BiLSTMImputer(\n        input_size=2,\n        hidden_size=config.hidden_size,\n        num_layers=config.num_layers,\n        dropout=config.dropout\n    ).to(config.device)\n    \n    print(f\"\\nModèle créé : {sum(p.numel() for p in model.parameters()):,} paramètres\")\n    \n    # PHASE 1 : Pré-entraînement\n    model, scaler = pretrain_model(\n        model, \n        X_tr, \n        clean_cols,\n        holed_cols,\n        config,\n        n_epochs_pretrain=n_epochs_pretrain\n    )\n    \n    # PHASE 2 : Fine-tuning\n    model = finetune_model(\n        model,\n        scaler,\n        X_tr,\n        Y_tr,\n        holed_cols,\n        config\n    )\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"ENTRAÎNEMENT COMPLET TERMINÉ !\")\n    print(\"=\"*60)\n    \n    return model, scaler","metadata":{"_uuid":"2b8ef619-66a9-475b-9df9-9dcf763483f4","_cell_guid":"bc37ce47-3f1c-4343-9c1b-5eb5b34dbe8f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-11-20T10:12:07.748417Z","iopub.execute_input":"2025-11-20T10:12:07.748728Z","iopub.status.idle":"2025-11-20T10:12:07.772445Z","shell.execute_reply.started":"2025-11-20T10:12:07.748683Z","shell.execute_reply":"2025-11-20T10:12:07.771676Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Entraînement","metadata":{"_uuid":"86bbcdaf-c8c7-45b1-861a-c591f2a852b3","_cell_guid":"d6e9b5f2-df20-40f6-9302-2bb1df71d48e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"config = Config()\n\n# Entraîner avec pré-entraînement\nmodel, scaler = train_with_pretraining(\n    X_tr=X_all,\n    Y_tr=Y_tr,\n    holed_cols=holed_cols,\n    clean_cols=clean_cols,\n    config=config,\n    n_epochs_pretrain=15  # Ajuster selon le temps disponible\n)","metadata":{"_uuid":"7219a456-88e1-4880-af96-27d7d9b172f1","_cell_guid":"c6202fc5-0640-44e5-b285-10a50757a959","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-20T10:12:07.773313Z","iopub.execute_input":"2025-11-20T10:12:07.773583Z","iopub.status.idle":"2025-11-20T12:50:50.285815Z","shell.execute_reply.started":"2025-11-20T10:12:07.773559Z","shell.execute_reply":"2025-11-20T12:50:50.285083Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nENTRAÎNEMENT AVEC PRÉ-ENTRAÎNEMENT\n============================================================\nCourbes complètes (pré-entraînement) : 57140\nCourbes à trous (fine-tuning) : 999\n============================================================\n\nModèle créé : 3,686,913 paramètres\n\n============================================================\nPHASE 1 : PRÉ-ENTRAÎNEMENT sur les 20k courbes complètes\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/4204779929.py:82: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  .fillna(method='bfill')\n/tmp/ipykernel_48/4204779929.py:83: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  .fillna(method='ffill'))\n","output_type":"stream"},{"name":"stdout","text":"\nCréation des masques réalistes (version optimisée)...\n","output_type":"stream"},{"name":"stderr","text":"Masking: 100%|██████████| 57140/57140 [01:57<00:00, 487.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n✓ Masqué 7,361,866 valeurs sur 60,396,980 (12.2%)\n✓ Distribution créée :\n  - Petits trous (1-12) : 2254274\n  - Moyens (13-47) : 25287\n  - Gros (48+) : 71706\n\nPré-entraînement : 51426 train, 5714 val\n\nDébut du pré-entraînement (15 epochs)...\nLearning rate réduit à 0.0001 pour stabilité\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15\n  Train Loss: 0.2432\n  Val Loss: 0.1911, Val MAE: 105.78\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/15\n  Train Loss: 0.1913\n  Val Loss: 0.1959, Val MAE: 107.04\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/15\n  Train Loss: 0.1809\n  Val Loss: 0.1798, Val MAE: 96.50\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/15\n  Train Loss: 0.1769\n  Val Loss: 0.1794, Val MAE: 99.13\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/15\n  Train Loss: 0.1728\n  Val Loss: 0.1698, Val MAE: 94.59\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6/15\n  Train Loss: 0.1688\n  Val Loss: 0.1632, Val MAE: 92.56\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7/15\n  Train Loss: 0.1649\n  Val Loss: 0.1602, Val MAE: 92.05\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8/15\n  Train Loss: 0.1626\n  Val Loss: 0.1601, Val MAE: 93.05\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9/15\n  Train Loss: 0.1616\n  Val Loss: 0.1585, Val MAE: 92.39\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10/15\n  Train Loss: 0.1599\n  Val Loss: 0.1645, Val MAE: 99.77\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 11/15\n  Train Loss: 0.1581\n  Val Loss: 0.1558, Val MAE: 89.89\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 12/15\n  Train Loss: 0.1570\n  Val Loss: 0.1568, Val MAE: 92.74\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 13/15\n  Train Loss: 0.1551\n  Val Loss: 0.1522, Val MAE: 88.12\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 14/15\n  Train Loss: 0.1540\n  Val Loss: 0.1518, Val MAE: 87.68\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/15\n  Train Loss: 0.1527\n  Val Loss: 0.1535, Val MAE: 93.75\n\n✓ Pré-entraînement terminé !\nLe modèle a appris les patterns généraux de consommation électrique\n\n============================================================\nPHASE 2 : FINE-TUNING sur les 1000 courbes à trous réels\n============================================================\nFine-tuning : 799 train, 200 val\n\nDébut du fine-tuning avec LR réduit...\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n  Train Loss: 0.1448\n  Val Loss: 0.1227, Val MAE: 81.97\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/50\n  Train Loss: 0.1421\n  Val Loss: 0.1229, Val MAE: 81.83\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3/50\n  Train Loss: 0.1414\n  Val Loss: 0.1222, Val MAE: 81.60\n  → New best model saved!\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/50\n  Train Loss: 0.1400\n  Val Loss: 0.1232, Val MAE: 81.53\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5/50\n  Train Loss: 0.1402\n  Val Loss: 0.1225, Val MAE: 80.91\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6/50\n  Train Loss: 0.1371\n  Val Loss: 0.1241, Val MAE: 82.32\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7/50\n  Train Loss: 0.1382\n  Val Loss: 0.1236, Val MAE: 81.04\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8/50\n  Train Loss: 0.1387\n  Val Loss: 0.1227, Val MAE: 82.16\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9/50\n  Train Loss: 0.1373\n  Val Loss: 0.1234, Val MAE: 82.40\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10/50\n  Train Loss: 0.1351\n  Val Loss: 0.1237, Val MAE: 81.91\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 11/50\n  Train Loss: 0.1366\n  Val Loss: 0.1240, Val MAE: 82.80\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 12/50\n  Train Loss: 0.1361\n  Val Loss: 0.1238, Val MAE: 81.77\n","output_type":"stream"},{"name":"stderr","text":"                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 13/50\n  Train Loss: 0.1356\n  Val Loss: 0.1237, Val MAE: 81.48\nEarly stopping at epoch 13\n\n✓ Fine-tuning terminé !\n\n============================================================\nENTRAÎNEMENT COMPLET TERMINÉ !\n============================================================\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Prédiction sur le X_test","metadata":{"_uuid":"0e42df87-b079-4a55-835a-acca621936d3","_cell_guid":"73450bf2-8e10-4da4-8dcb-a8bd129f7097","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"X_test_holed_cols = [c for c in X_test.columns if c.startswith(\"holed\")]\nX_test_imputed = predict(model, X_test[X_test_holed_cols], scaler, config)","metadata":{"_uuid":"f7bdbf46-d65c-425d-94b0-55b0df4d7283","_cell_guid":"069f9aea-b541-41f8-9d96-75554bc4e746","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-20T12:50:50.286623Z","iopub.execute_input":"2025-11-20T12:50:50.287057Z","iopub.status.idle":"2025-11-20T12:50:54.550858Z","shell.execute_reply.started":"2025-11-20T12:50:50.287036Z","shell.execute_reply":"2025-11-20T12:50:54.550064Z"}},"outputs":[{"name":"stderr","text":"Predicting: 100%|██████████| 16/16 [00:04<00:00,  3.83it/s]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"X_test_imputed","metadata":{"_uuid":"d2d4fd5f-4120-4e5d-aec4-b590c94306ee","_cell_guid":"2bc9fe52-0bba-4777-9bf7-bfe3b61ff698","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T12:50:54.551701Z","iopub.execute_input":"2025-11-20T12:50:54.551967Z","iopub.status.idle":"2025-11-20T12:50:54.587582Z","shell.execute_reply.started":"2025-11-20T12:50:54.551943Z","shell.execute_reply":"2025-11-20T12:50:54.587053Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                         holed_1     holed_2     holed_3     holed_4  \\\nHorodate                                                               \n2023-01-09 00:00:00  1061.000000  210.594376  171.684357   20.000000   \n2023-01-09 00:30:00  1041.000000  160.155075   99.000000   42.000000   \n2023-01-09 01:00:00   995.000000  133.136932  105.000000   19.000000   \n2023-01-09 01:30:00   998.000000  118.447319   79.737755   34.000000   \n2023-01-09 02:00:00   950.417297  107.814857  107.000000   21.000000   \n...                          ...         ...         ...         ...   \n2023-01-30 22:00:00  1724.000000   97.000000   48.000000  124.858604   \n2023-01-30 22:30:00  1677.000000   63.000000   40.000000  122.092903   \n2023-01-30 23:00:00  1648.000000   60.000000   47.003956  116.816841   \n2023-01-30 23:30:00  1490.000000   58.000000   73.523216  113.325645   \n2023-01-31 00:00:00  1543.000000   46.000000   55.000000   20.000000   \n\n                        holed_5  holed_6     holed_7     holed_8  holed_9  \\\nHorodate                                                                    \n2023-01-09 00:00:00  223.038864    148.0   70.000000  183.000000     88.0   \n2023-01-09 00:30:00  138.000000    164.0   59.000000  170.000000     83.0   \n2023-01-09 01:00:00  145.000000     93.0  119.000000  403.000000     60.0   \n2023-01-09 01:30:00  270.000000    126.0  505.000000  489.000000     66.0   \n2023-01-09 02:00:00  309.000000   1279.0  395.000000  288.000000     46.0   \n...                         ...      ...         ...         ...      ...   \n2023-01-30 22:00:00  424.000000    632.0  125.000000  177.169586    227.0   \n2023-01-30 22:30:00  435.000000    610.0  148.375870  180.411407    259.0   \n2023-01-30 23:00:00  428.000000    535.0  166.793823  165.000000    286.0   \n2023-01-30 23:30:00  398.000000    506.0   96.000000  140.000000    195.0   \n2023-01-31 00:00:00  403.000000    532.0  495.000000  164.000000    164.0   \n\n                       holed_10  ...  holed_991   holed_992  holed_993  \\\nHorodate                         ...                                     \n2023-01-09 00:00:00  163.951874  ...       38.0  168.000000       32.0   \n2023-01-09 00:30:00  111.364815  ...       27.0   82.000000       48.0   \n2023-01-09 01:00:00   87.745766  ...       37.0  102.000000       44.0   \n2023-01-09 01:30:00   78.057671  ...       13.0   78.000000       33.0   \n2023-01-09 02:00:00   74.030022  ...       62.0  107.000000       36.0   \n...                         ...  ...        ...         ...        ...   \n2023-01-30 22:00:00   34.000000  ...      146.0  280.694275      137.0   \n2023-01-30 22:30:00   26.000000  ...       66.0  264.632507      155.0   \n2023-01-30 23:00:00   11.781667  ...       44.0  242.556473      131.0   \n2023-01-30 23:30:00   35.000000  ...       53.0  219.161179       86.0   \n2023-01-31 00:00:00   22.000000  ...       60.0  127.000000       73.0   \n\n                      holed_994  holed_995   holed_996   holed_997  holed_998  \\\nHorodate                                                                        \n2023-01-09 00:00:00  285.081451      445.0  262.760101  174.697006      767.0   \n2023-01-09 00:30:00  247.368912      492.0  207.000000  121.764168      773.0   \n2023-01-09 01:00:00  227.160172      461.0  218.000000   96.690773      613.0   \n2023-01-09 01:30:00  218.152115      491.0  104.000000   85.266655      691.0   \n2023-01-09 02:00:00  214.814774      451.0   97.000000   79.555260      844.0   \n...                         ...        ...         ...         ...        ...   \n2023-01-30 22:00:00  669.000000      881.0  596.000000  147.000000      502.0   \n2023-01-30 22:30:00  909.000000      985.0  681.000000   88.000000      741.0   \n2023-01-30 23:00:00  973.000000      940.0  206.000000   65.000000      809.0   \n2023-01-30 23:30:00  853.000000      833.0  342.000000   42.000000      822.0   \n2023-01-31 00:00:00  674.000000      867.0  318.000000   56.000000      871.0   \n\n                     holed_999  holed_1000  \nHorodate                                    \n2023-01-09 00:00:00     2029.0  129.000000  \n2023-01-09 00:30:00     1698.0  113.000000  \n2023-01-09 01:00:00     1737.0   92.391991  \n2023-01-09 01:30:00      994.0  135.000000  \n2023-01-09 02:00:00     1044.0  102.310936  \n...                        ...         ...  \n2023-01-30 22:00:00     1599.0   49.613087  \n2023-01-30 22:30:00     1887.0   78.000000  \n2023-01-30 23:00:00     1882.0   57.000000  \n2023-01-30 23:30:00     2534.0   49.000000  \n2023-01-31 00:00:00     2011.0   42.000000  \n\n[1057 rows x 1000 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>holed_1</th>\n      <th>holed_2</th>\n      <th>holed_3</th>\n      <th>holed_4</th>\n      <th>holed_5</th>\n      <th>holed_6</th>\n      <th>holed_7</th>\n      <th>holed_8</th>\n      <th>holed_9</th>\n      <th>holed_10</th>\n      <th>...</th>\n      <th>holed_991</th>\n      <th>holed_992</th>\n      <th>holed_993</th>\n      <th>holed_994</th>\n      <th>holed_995</th>\n      <th>holed_996</th>\n      <th>holed_997</th>\n      <th>holed_998</th>\n      <th>holed_999</th>\n      <th>holed_1000</th>\n    </tr>\n    <tr>\n      <th>Horodate</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2023-01-09 00:00:00</th>\n      <td>1061.000000</td>\n      <td>210.594376</td>\n      <td>171.684357</td>\n      <td>20.000000</td>\n      <td>223.038864</td>\n      <td>148.0</td>\n      <td>70.000000</td>\n      <td>183.000000</td>\n      <td>88.0</td>\n      <td>163.951874</td>\n      <td>...</td>\n      <td>38.0</td>\n      <td>168.000000</td>\n      <td>32.0</td>\n      <td>285.081451</td>\n      <td>445.0</td>\n      <td>262.760101</td>\n      <td>174.697006</td>\n      <td>767.0</td>\n      <td>2029.0</td>\n      <td>129.000000</td>\n    </tr>\n    <tr>\n      <th>2023-01-09 00:30:00</th>\n      <td>1041.000000</td>\n      <td>160.155075</td>\n      <td>99.000000</td>\n      <td>42.000000</td>\n      <td>138.000000</td>\n      <td>164.0</td>\n      <td>59.000000</td>\n      <td>170.000000</td>\n      <td>83.0</td>\n      <td>111.364815</td>\n      <td>...</td>\n      <td>27.0</td>\n      <td>82.000000</td>\n      <td>48.0</td>\n      <td>247.368912</td>\n      <td>492.0</td>\n      <td>207.000000</td>\n      <td>121.764168</td>\n      <td>773.0</td>\n      <td>1698.0</td>\n      <td>113.000000</td>\n    </tr>\n    <tr>\n      <th>2023-01-09 01:00:00</th>\n      <td>995.000000</td>\n      <td>133.136932</td>\n      <td>105.000000</td>\n      <td>19.000000</td>\n      <td>145.000000</td>\n      <td>93.0</td>\n      <td>119.000000</td>\n      <td>403.000000</td>\n      <td>60.0</td>\n      <td>87.745766</td>\n      <td>...</td>\n      <td>37.0</td>\n      <td>102.000000</td>\n      <td>44.0</td>\n      <td>227.160172</td>\n      <td>461.0</td>\n      <td>218.000000</td>\n      <td>96.690773</td>\n      <td>613.0</td>\n      <td>1737.0</td>\n      <td>92.391991</td>\n    </tr>\n    <tr>\n      <th>2023-01-09 01:30:00</th>\n      <td>998.000000</td>\n      <td>118.447319</td>\n      <td>79.737755</td>\n      <td>34.000000</td>\n      <td>270.000000</td>\n      <td>126.0</td>\n      <td>505.000000</td>\n      <td>489.000000</td>\n      <td>66.0</td>\n      <td>78.057671</td>\n      <td>...</td>\n      <td>13.0</td>\n      <td>78.000000</td>\n      <td>33.0</td>\n      <td>218.152115</td>\n      <td>491.0</td>\n      <td>104.000000</td>\n      <td>85.266655</td>\n      <td>691.0</td>\n      <td>994.0</td>\n      <td>135.000000</td>\n    </tr>\n    <tr>\n      <th>2023-01-09 02:00:00</th>\n      <td>950.417297</td>\n      <td>107.814857</td>\n      <td>107.000000</td>\n      <td>21.000000</td>\n      <td>309.000000</td>\n      <td>1279.0</td>\n      <td>395.000000</td>\n      <td>288.000000</td>\n      <td>46.0</td>\n      <td>74.030022</td>\n      <td>...</td>\n      <td>62.0</td>\n      <td>107.000000</td>\n      <td>36.0</td>\n      <td>214.814774</td>\n      <td>451.0</td>\n      <td>97.000000</td>\n      <td>79.555260</td>\n      <td>844.0</td>\n      <td>1044.0</td>\n      <td>102.310936</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2023-01-30 22:00:00</th>\n      <td>1724.000000</td>\n      <td>97.000000</td>\n      <td>48.000000</td>\n      <td>124.858604</td>\n      <td>424.000000</td>\n      <td>632.0</td>\n      <td>125.000000</td>\n      <td>177.169586</td>\n      <td>227.0</td>\n      <td>34.000000</td>\n      <td>...</td>\n      <td>146.0</td>\n      <td>280.694275</td>\n      <td>137.0</td>\n      <td>669.000000</td>\n      <td>881.0</td>\n      <td>596.000000</td>\n      <td>147.000000</td>\n      <td>502.0</td>\n      <td>1599.0</td>\n      <td>49.613087</td>\n    </tr>\n    <tr>\n      <th>2023-01-30 22:30:00</th>\n      <td>1677.000000</td>\n      <td>63.000000</td>\n      <td>40.000000</td>\n      <td>122.092903</td>\n      <td>435.000000</td>\n      <td>610.0</td>\n      <td>148.375870</td>\n      <td>180.411407</td>\n      <td>259.0</td>\n      <td>26.000000</td>\n      <td>...</td>\n      <td>66.0</td>\n      <td>264.632507</td>\n      <td>155.0</td>\n      <td>909.000000</td>\n      <td>985.0</td>\n      <td>681.000000</td>\n      <td>88.000000</td>\n      <td>741.0</td>\n      <td>1887.0</td>\n      <td>78.000000</td>\n    </tr>\n    <tr>\n      <th>2023-01-30 23:00:00</th>\n      <td>1648.000000</td>\n      <td>60.000000</td>\n      <td>47.003956</td>\n      <td>116.816841</td>\n      <td>428.000000</td>\n      <td>535.0</td>\n      <td>166.793823</td>\n      <td>165.000000</td>\n      <td>286.0</td>\n      <td>11.781667</td>\n      <td>...</td>\n      <td>44.0</td>\n      <td>242.556473</td>\n      <td>131.0</td>\n      <td>973.000000</td>\n      <td>940.0</td>\n      <td>206.000000</td>\n      <td>65.000000</td>\n      <td>809.0</td>\n      <td>1882.0</td>\n      <td>57.000000</td>\n    </tr>\n    <tr>\n      <th>2023-01-30 23:30:00</th>\n      <td>1490.000000</td>\n      <td>58.000000</td>\n      <td>73.523216</td>\n      <td>113.325645</td>\n      <td>398.000000</td>\n      <td>506.0</td>\n      <td>96.000000</td>\n      <td>140.000000</td>\n      <td>195.0</td>\n      <td>35.000000</td>\n      <td>...</td>\n      <td>53.0</td>\n      <td>219.161179</td>\n      <td>86.0</td>\n      <td>853.000000</td>\n      <td>833.0</td>\n      <td>342.000000</td>\n      <td>42.000000</td>\n      <td>822.0</td>\n      <td>2534.0</td>\n      <td>49.000000</td>\n    </tr>\n    <tr>\n      <th>2023-01-31 00:00:00</th>\n      <td>1543.000000</td>\n      <td>46.000000</td>\n      <td>55.000000</td>\n      <td>20.000000</td>\n      <td>403.000000</td>\n      <td>532.0</td>\n      <td>495.000000</td>\n      <td>164.000000</td>\n      <td>164.0</td>\n      <td>22.000000</td>\n      <td>...</td>\n      <td>60.0</td>\n      <td>127.000000</td>\n      <td>73.0</td>\n      <td>674.000000</td>\n      <td>867.0</td>\n      <td>318.000000</td>\n      <td>56.000000</td>\n      <td>871.0</td>\n      <td>2011.0</td>\n      <td>42.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>1057 rows × 1000 columns</p>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"X_test_imputed.shape","metadata":{"_uuid":"08790812-5524-49b8-adf4-217407107384","_cell_guid":"1ed57216-7017-462c-8916-afb6f809748b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-20T12:50:54.588278Z","iopub.execute_input":"2025-11-20T12:50:54.588510Z","iopub.status.idle":"2025-11-20T12:50:54.593114Z","shell.execute_reply.started":"2025-11-20T12:50:54.588494Z","shell.execute_reply":"2025-11-20T12:50:54.592425Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(1057, 1000)"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"pd.DataFrame(X_test_imputed, \n             index=X_test.index, \n             columns=X_test_holed_cols).to_csv(\"predictions_bilstm.csv\")","metadata":{"_uuid":"a26ba7a9-ee15-4f76-8213-658452bb653e","_cell_guid":"cf17ac0c-491f-40d8-8921-e841921b2607","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-20T12:50:54.593921Z","iopub.execute_input":"2025-11-20T12:50:54.594170Z","iopub.status.idle":"2025-11-20T12:50:55.986138Z","shell.execute_reply.started":"2025-11-20T12:50:54.594154Z","shell.execute_reply":"2025-11-20T12:50:55.985522Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Ensemble learning","metadata":{}},{"cell_type":"code","source":"def ensemble_predictions(pred_bilstm, pred_interp, alpha=0.7, \n                        handle_nan='interp'):\n    \"\"\"\n    Combiner les prédictions du BiLSTM et de l'interpolation linéaire\n    \n    Args:\n        pred_bilstm: prédictions du modèle BiLSTM\n        pred_interp: prédictions de l'interpolation linéaire\n        alpha: poids du BiLSTM (1-alpha = poids de l'interpolation)\n               alpha=1.0 → 100% BiLSTM\n               alpha=0.5 → moyenne 50/50\n               alpha=0.0 → 100% interpolation\n        handle_nan: comment gérer les NaN du BiLSTM\n                    'interp' : remplacer par interpolation\n                    'zero' : remplacer par 0\n                    'drop' : ignorer (garder NaN)\n    \n    Returns:\n        predictions_ensemble: DataFrame avec prédictions combinées\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"Ensemble avec alpha={alpha:.2f}\")\n    print(f\"  → {alpha*100:.0f}% BiLSTM + {(1-alpha)*100:.0f}% Interpolation\")\n    print(f\"{'='*60}\")\n    \n    # Vérifier les dimensions\n    assert pred_bilstm.shape == pred_interp.shape, \\\n        f\"Shapes incompatibles : {pred_bilstm.shape} vs {pred_interp.shape}\"\n    \n    # Gérer les NaN du BiLSTM\n    nan_count_bilstm = pred_bilstm.isna().sum().sum()\n    if nan_count_bilstm > 0:\n        print(f\"BiLSTM contient {nan_count_bilstm} NaN\")\n        \n        if handle_nan == 'interp':\n            print(f\"  → Remplacement par l'interpolation linéaire\")\n            pred_bilstm_clean = pred_bilstm.copy()\n            mask_nan = pred_bilstm_clean.isna()\n            pred_bilstm_clean[mask_nan] = pred_interp[mask_nan]\n        elif handle_nan == 'zero':\n            print(f\"  → Remplacement par 0\")\n            pred_bilstm_clean = pred_bilstm.fillna(0)\n        else:\n            pred_bilstm_clean = pred_bilstm\n    else:\n        pred_bilstm_clean = pred_bilstm\n        print(\"✓ BiLSTM ne contient pas de NaN\")\n    \n    # Ensemble pondéré\n    predictions_ensemble = alpha * pred_bilstm_clean + (1 - alpha) * pred_interp\n    \n    # Vérification finale\n    final_nan = predictions_ensemble.isna().sum().sum()\n    if final_nan > 0:\n        print(f\"{final_nan} NaN dans l'ensemble final\")\n        predictions_ensemble = predictions_ensemble.fillna(method='ffill').fillna(method='bfill')\n        print(\"  → NaN remplacés par forward/backward fill\")\n    \n    print(f\"\\n✓ Ensemble créé : {predictions_ensemble.shape}\")\n    \n    return predictions_ensemble","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T12:50:55.986875Z","iopub.execute_input":"2025-11-20T12:50:55.987062Z","iopub.status.idle":"2025-11-20T12:50:55.994178Z","shell.execute_reply.started":"2025-11-20T12:50:55.987047Z","shell.execute_reply":"2025-11-20T12:50:55.993646Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Prédictions\npred_bilstm = X_test_imputed\npred_interp = predict_interpolation(X_test[X_test_holed_cols])\n\n# Ensemble avec alpha=0.7 (recommandé)\nensemble_90 = ensemble_predictions(pred_bilstm, pred_interp, alpha=0.9)\nensemble_90.to_csv(\"submission_best90.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T12:50:55.995125Z","iopub.execute_input":"2025-11-20T12:50:55.995364Z","iopub.status.idle":"2025-11-20T12:50:57.737240Z","shell.execute_reply.started":"2025-11-20T12:50:55.995343Z","shell.execute_reply":"2025-11-20T12:50:57.736677Z"}},"outputs":[{"name":"stdout","text":"Prédiction par interpolation linéaire...\n✓ Interpolation terminée : 1000 courbes imputées\n\n============================================================\nEnsemble avec alpha=0.90\n  → 90% BiLSTM + 10% Interpolation\n============================================================\n✓ BiLSTM ne contient pas de NaN\n\n✓ Ensemble créé : (1057, 1000)\n","output_type":"stream"}],"execution_count":21}]}